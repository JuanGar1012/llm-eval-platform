Metadata-Version: 2.4
Name: llm-eval-platform
Version: 0.1.0
Summary: Local-first LLM evaluation platform with CLI/API workflows.
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: fastapi<1.0.0,>=0.116.0
Requires-Dist: httpx<1.0.0,>=0.28.0
Requires-Dist: jsonschema<5.0.0,>=4.23.0
Requires-Dist: pydantic<3.0.0,>=2.11.0
Requires-Dist: PyYAML<7.0.0,>=6.0.0
Requires-Dist: sqlalchemy<3.0.0,>=2.0.0
Requires-Dist: typer<1.0.0,>=0.16.0
Requires-Dist: uvicorn<1.0.0,>=0.35.0
Provides-Extra: dev
Requires-Dist: pytest<9.0.0,>=8.4.0; extra == "dev"

# LLM Eval Platform (Local-Only, Zero API Cost)

Production-style starter for evaluating prompt/model/retrieval variants with regression gates before release.

## What This Scaffold Includes

- Python-first architecture with strong type hints and modular packages.
- FastAPI endpoints for dataset registration, run execution, comparison, and report export.
- Typer CLI with required commands:
  - `register-dataset`
  - `run-eval`
  - `compare-runs`
  - `export-report`
  - `db-check`
- SQLite storage for datasets, runs, item-level scores, and gate outcomes.
- Rule-based scoring:
  - exact match
  - keyword coverage
  - structured JSON output schema validity
- Optional local LLM-judge mode (Ollama model only).
- Regression gates (minimum metrics + max drop from baseline).
- Markdown + JSON reporting with per-tag breakdown and pass/fail gate status.
- Portfolio-ready metrics snapshot JSON artifact.

## Folder Structure

```text
.
|-- configs/
|   |-- baseline.yaml
|   `-- candidate.yaml
|-- datasets/
|   `-- sample_benchmark.jsonl
|-- reports/
|-- src/llm_eval_platform/
|   |-- api.py
|   |-- cli.py
|   |-- config.py
|   |-- service.py
|   |-- domain/models.py
|   |-- ingestion/registry.py
|   |-- runner/{experiment.py, ollama_client.py, retrieval.py}
|   |-- scoring/{metrics.py, gates.py}
|   |-- storage/{db.py, repository.py}
|   `-- reporting/exporter.py
`-- tests/
    |-- test_metrics.py
    `-- test_gates.py
```

## Prerequisites (Windows PowerShell)

1. Python 3.10+ installed.
2. Ollama installed and running (`http://localhost:11434`).
3. Local model pulled, for example:
   ```powershell
   ollama pull llama3.2:3b
   ```

## Setup

```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
pip install -e .[dev]
```

## CLI Workflow

### 1) Register Dataset

```powershell
llm-eval register-dataset --dataset-name sample_benchmark --version v1 --path datasets\sample_benchmark.jsonl
```

Expected output example:

```text
registered dataset sample_benchmark:v1 (4 items)
```

### 2) Run Baseline

```powershell
llm-eval run-eval --config configs\baseline.yaml
```

Expected output example:

```text
completed run 2f2c2d580f1e6d3a-v1 status=completed
{
  "exact_match": 0.5,
  "keyword_coverage": 0.67,
  "schema_valid": 0.75,
  "llm_judge_score": null,
  "sample_count": 4
}
```

### 3) Run Candidate (set baseline in config first)

Update `configs/candidate.yaml`:
- Set `gates.baseline_run_id` to your baseline run id.

Then run:

```powershell
llm-eval run-eval --config configs\candidate.yaml
```

### 4) Compare Runs

```powershell
llm-eval compare-runs --baseline-run-id <BASELINE_RUN_ID> --candidate-run-id <CANDIDATE_RUN_ID>
```

### 5) Export Report + Portfolio Snapshot

```powershell
llm-eval export-report --run-id <CANDIDATE_RUN_ID> --baseline-run-id <BASELINE_RUN_ID> --output-dir reports
```

Artifacts:
- `reports/<RUN_ID>.report.md`
- `reports/<RUN_ID>.report.json`
- `reports/<RUN_ID>.metrics_snapshot.json`

### 6) Check DB Schema Version

```powershell
llm-eval db-check
```

Expected output example:

```json
{
  "db_path": "C:\\Users\\you\\project\\llm_eval.db",
  "schema_version": 1
}
```

## API Workflow

Start API:

```powershell
uvicorn llm_eval_platform.api:app --reload
```

Endpoints:
- `GET /health` (includes DB schema version)
- `POST /datasets/register`
- `POST /runs`
- `GET /runs/{run_id}`
- `GET /runs/{run_id}/results`
- `POST /compare`
- `POST /reports/export`

## Config Files

- Baseline config: `configs/baseline.yaml`
- Candidate config: `configs/candidate.yaml`

Both use this schema:

```yaml
variant:
  name: string
  dataset_name: string
  dataset_version: string
  model_name: string
  prompt_version: string
  prompt_template: "Task: {prompt}"
  retrieval_enabled: false
  llm_judge_enabled: false
  llm_judge_model: null
  seed: 42
  temperature: 0.0
gates:
  baseline_run_id: null
  min_metric:
    exact_match: 0.2
  max_drop_from_baseline:
    exact_match: 0.05
```

## Where To Plug In Real Data and Thresholds

- Replace placeholder benchmark with your real JSONL prompts in `datasets/`.
- Register your dataset version via `register-dataset`.
- Set your acceptance thresholds in:
  - `gates.min_metric`
  - `gates.max_drop_from_baseline`
- Add/adjust tags (`domain`, `difficulty`, `safety`, `format`) per row for richer per-tag reporting.

## Run Tests

```powershell
pytest -q
```
