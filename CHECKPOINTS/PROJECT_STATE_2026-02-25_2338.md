# PROJECT_STATE.md

## Goal
- Build a local-first, zero-API-cost LLM evaluation and release-safety platform with production-style reproducibility, regression gating, diagnostics, and observability.

## Non-goals
- No managed cloud services.
- No paid model APIs.
- No heavy frontend build pipeline requirement for dashboard operation.

## Current architecture
- Backend: FastAPI + service/repository layers.
- Runner: local Ollama inference and optional local judge.
- Storage: SQLite with additive migration logic.
- UI: React UMD + Tailwind CDN dashboard served by FastAPI.
- Reports: Markdown/JSON + metrics snapshot artifacts.

## Data model (current)
- `datasets`: versioned dataset registry.
- `runs`: run metadata, gate decision, fingerprints, release status, latency/tokens/cost estimates.
- `item_results`: per-sample outputs, scores, misses, schema errors, latency/tokens.
- `run_tag_metrics`: per-tag aggregate metrics for slice analysis.
- `schema_metadata`: schema version tracking.

## Key commands
- Setup: `python -m venv .venv`, `.venv\Scripts\Activate.ps1`, `pip install -e .[dev]`
- Tests: `pytest -q`
- CLI DB check: `llm-eval db-check`
- API: `uvicorn llm_eval_platform.api:app --reload`
- UI: open `http://127.0.0.1:8000/ui`

## Files modified (high-level)
- `src/llm_eval_platform/config.py`: reproducibility fingerprint generation.
- `src/llm_eval_platform/storage/db.py`: schema extensions and additive migration logic.
- `src/llm_eval_platform/storage/repository.py`: persistence/read APIs for new run/item/tag fields.
- `src/llm_eval_platform/runner/experiment.py`: run instrumentation, fingerprints, drift alerts, release status, latency/tokens.
- `src/llm_eval_platform/analysis.py`: trends, volatility, drift alerts, failure ranking/clustering.
- `src/llm_eval_platform/service.py`: trends and failure analysis services.
- `src/llm_eval_platform/api.py`: new analysis endpoints and local model list endpoint.
- `src/llm_eval_platform/runner/ollama_client.py`: local model listing via `/api/tags`.
- `src/llm_eval_platform/web/static/app.jsx`: run-first cockpit enhancements and analysis visualization.
- `tests/test_api.py`: endpoint coverage updates.
- `README.md`: route/features updates.
- `AGENTS.md`: context management policy.

## Completed items
- Deterministic run-level fingerprints and experiment signature implemented and stored.
- Release status layer added (`APPROVED`, `DRIFT_WARNING`, `BLOCKED`).
- Observability metrics added: duration, avg latency, p95, token estimates, cost estimate.
- Drift characterization added: trends, volatility, drift alerts.
- Failure surfacing added: worst samples and failure clustering endpoints.
- Tag-level slice metrics persisted and exposed.
- Local model discovery endpoint added and wired to UI run execution form.
- UI upgraded with pin baseline/candidate, release decision panel, trends, diagnostics previews.
- Test suite passing (`11 passed`).

## Open tasks (priority order)
1. Add dedicated run detail route in UI (`/ui/runs/:runId`) with deep failure exploration.
2. Persist and expose drift alerts as first-class DB rows (currently in `runs.metadata`).
3. Add richer threshold visualization and per-metric allowed-drop overlays in UI compare page.
4. Add CLI parity commands for trends/failures/release-decision.
5. Harden token estimation with model-specific tokenizers when available.

## Known issues / edge cases
- Legacy DBs created before new columns rely on additive migration; destructive or type-change migrations are not handled.
- Token estimates use heuristic (`chars/4`), not tokenizer-accurate.
- Local model endpoint depends on Ollama availability; returns empty model list when unavailable.
- UI uses CDN-delivered React/Tailwind in-browser; production bundling is not yet applied.

## Next 3 actions
- [ ] Add run detail page route and deep failure drilldown UX.
- [ ] Persist drift alerts into dedicated table and expose historical alert timeline.
- [ ] Add release readiness export section including blockers/warnings in report artifacts.
